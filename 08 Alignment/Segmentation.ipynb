{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and Alignment of Ebook\n",
    "In this exercise, we are going to segment an ebook and align the segments to create a translation memory. We will do this in a unique way. We will predict the most probable alignment by leveraging a BLEU score of a machine translation. If a source segment's machine translation has a high BLEU score, it's likely that it matches with the corresponding segment from the translated ebook.\n",
    "\n",
    "# Download Calibre\n",
    "From Calibre's website: \"calibre is a powerful and easy to use e-book manager. Users say it’s outstanding and a must-have. It’ll allow you to do nearly everything and it takes things a step beyond normal e-book software. It’s also completely free and open source and great for both casual users and computer experts.\"\n",
    "Download and install Calibre [here](https://calibre-ebook.com/download).\n",
    "\n",
    "# Import and Convert Ebooks to TXT\n",
    "Press `a` to add a book to Calibre. Select the book you'd like to add. The book's title and details will appear in the list. Select the book and click **Convert books**. In the top right of the conversion screen, change the **Output format** to TXT. Click **OK**. Wait for the job to complete. In the right preview panel, the available formats will be displayed. You can also **Click to open** the **Path**. For each ebook, import it into Calibre and convert it to txt.\n",
    "\n",
    "# Prepare for Segmentation\n",
    "Clone the srx_segmenter GitHub repo with this line of code (if you have git)\n",
    "\n",
    "`git clone https://github.com/nicklambson/srx_segmenter.git`\n",
    "\n",
    "If you don't have git, just [download](https://github.com/nicklambson/srx_segmenter/archive/master.zip) and unzip the zip into your project folder (in the same directory as your python script).\n",
    "\n",
    "Also be sure to install `regex`, an alternative to the core library `re` module.\n",
    "\n",
    "`pip install regex`\n",
    "\n",
    "If you need to install `regex` in your jupyter notebook, add this block of code to your notebook and run it:\n",
    "\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install regex\n",
    "    \n",
    "The srx segmentation rules are stored locale-by-locale in the 0_srx_rules folder. They were downloaded from memoQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_tools import parse, SrxSegmenter\n",
    "\n",
    "# Set the name of the text file you'd like to segment.\n",
    "TEXT = r\"3_extracted_text\\Como ganar amigos e influir sob - Dale Carnegie.txt\"\n",
    "\n",
    "# Set the locale of the file\n",
    "# This will be used in the segmented filename\n",
    "# It is also important in order to locate the SRX file for that locale\n",
    "\n",
    "LOCALE = \"es\"\n",
    "LOCALE_MAP = {\"en_US\": \"English (United States)\",\n",
    "              \"zh_CN\": \"Chinese (PRC)\",\n",
    "              \"es\": \"Spanish\"}\n",
    "\n",
    "# Locale code for your segmentation rules\n",
    "SRX_PATH = r\"0_srx_rules/\" + LOCALE + \".srx\"\n",
    "\n",
    "# Parse the srx file\n",
    "SRX = parse(srx_filepath=SRX_PATH)\n",
    "\n",
    "# Get the language rules from the SRX file\n",
    "LANGUAGE_RULES = SRX[LOCALE_MAP[LOCALE]]\n",
    "\n",
    "# The filename of your resulting segmented file\n",
    "SEGMENTED_FILEPATH = r\"4_segmented_text/\" + \"segmented_\" + LOCALE + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Segmentation\n",
    "We pass the language rules to the segmenter along with the content. It returns to us both segments and whitespace, but we will only use the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file to read from and the file to write to\n",
    "with open(TEXT, \"r\", encoding=\"utf-8\") as r:\n",
    "    with open(SEGMENTED_FILEPATH, \"w\", encoding=\"utf-8\") as w:\n",
    "        \n",
    "        # Read the content of the text file\n",
    "        content = r.read()\n",
    "        segmenter = SrxSegmenter(LANGUAGE_RULES, content)\n",
    "        \n",
    "        # Segment the text into segments and whitespace\n",
    "        segments, whitespace = segmenter.extract()\n",
    "        \n",
    "        # Write the segments to a new file\n",
    "        for s in segments:\n",
    "            w.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Open your segmented files. The segmentation is probably not perfect. Are there any ways you would improve the segmentation rules? If you can figure it out, you can add some rules to the srx file and improve the segmentation.  \n",
    "\n",
    "You may notice some extra text or recurring patterns that make the text a little unclean. You can remove those by searching in the text with regular expressions. Clean up the files as much as possible before moving to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translate the Source Text\n",
    "\n",
    "Next, we are going to generate a machine translation of the source text, line by line. After we machine translate the source text, we can get the BLEU score of the MT and use it to predict which segments should be aligned together.\n",
    "\n",
    "First, go to the [Baidu Translation API](http://api.fanyi.baidu.com/api/trans/product/desktop) to sign up for an API key. Paste your APP_ID and SECRET_KEY in the appropriate variables below.\n",
    "\n",
    "[Languages Available](http://api.fanyi.baidu.com/api/trans/product/apidoc#languageList)\n",
    "\n",
    "[API Reference](http://api.fanyi.baidu.com/api/trans/product/apidoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import hashlib\n",
    "import urllib\n",
    "import random\n",
    "import traceback\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set App ID and Secret Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own App ID and Secret Key\n",
    "APP_ID = '1234567890'\n",
    "SECRET_KEY = '1234567890'\n",
    "\n",
    "BASE_URL = 'api.fanyi.baidu.com'\n",
    "API_URL = r'/api/trans/vip/translate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Translate Function\n",
    "You can change the timeout time if you'd like. This will allow 20 seconds wait time before timing out on an http request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(from_lang, to_lang, query_text):\n",
    "    httpClient = None\n",
    "    url = get_url(from_lang, to_lang, query_text)\n",
    "    try:\n",
    "        httpClient = http.client.HTTPConnection(BASE_URL, timeout=20)\n",
    "        httpClient.request('GET', url)\n",
    "        response = httpClient.getresponse()\n",
    "        result = json.loads(response.read())\n",
    "        print(result)\n",
    "        result_list = []\n",
    "        for ret in result[\"trans_result\"]:\n",
    "            result_list.append(ret[\"dst\"])\n",
    "        trans_result = \"\".join(result_list)\n",
    "        return trans_result \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if httpClient:\n",
    "            httpClient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the get URL function.\n",
    "This sets up the URL to be sent to the http client, including salt and hashing. Just leave the settings as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(from_lang, to_lang, query_text):\n",
    "    salt = random.randint(32768, 65536)\n",
    "    sign = str.encode(APP_ID + query_text + str(salt) + SECRET_KEY)\n",
    "    m1 = hashlib.new('md5')\n",
    "    m1.update(sign)\n",
    "    sign = m1.hexdigest()\n",
    "    url = API_URL +'?appid=' + APP_ID + '&q=' + urllib.parse.quote(query_text) + '&from=' + from_lang + '&to=' + to_lang + '&salt=' + str(salt) + '&sign=' + sign\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the language code for Spanish for Baidu translate is spa, not es\n",
    "FROM = \"en\"\n",
    "TO = \"spa\"\n",
    "SOURCE_FILE = r\"4_segmented_text/segmented_en_US.txt\"\n",
    "MT_FILEPATH = r\"5_machine_translation/machineTranslated_\" + TO + \".txt\"\n",
    "\n",
    "# Change the limit to stop the process after an amount of lines\n",
    "count = 0\n",
    "limit = 2000\n",
    "\n",
    "with open(SOURCE_FILE, \"r\", encoding=\"utf-8\") as f, /\n",
    "     open(MT_FILEPATH, \"w\", encoding=\"utf-8\") as w:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line == \"\\n\":\n",
    "            w.write(\"\\n\")\n",
    "        else:\n",
    "            trans_result = translate(FROM, TO, line)\n",
    "            w.write(trans_result + \"\\n\")\n",
    "\n",
    "        # The translation will stop if the count exceeds the limit.\n",
    "        if count > limit:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        \n",
    "        # sleep for 5 seconds between each translation request\n",
    "        # This will help resolve \"invalid access limit\" error\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu Align\n",
    "Let's install bleualign. Don't use the bluealign from pypi, it's not updated. Just install it with pip from GitHub. \n",
    "\n",
    "`pip install git+git://github.com/rsennrich/Bleualign.git`\n",
    "\n",
    "Paste the filenames of your source file, translated file, and machine translation file into the below variables. srctotarget is the variable for the machine translated file. output-src and output-target are IO wrappers that will write information to the respective filename.\n",
    "\n",
    "The last line of code writes the results to the aligned source and target filenames.\n",
    "\n",
    "Copy three files into a folder called `6_for_alignment`: source segmented file, original segmented translation, and your machine translation.\n",
    "\n",
    "If you limited your machine translation to 2000 lines, do the following to your files in `6_for_alignment`:\n",
    " - In the source file, delete any lines above 2000\n",
    " - In the original segmented translation, locate the line represented by the last line of your machine translation. This is a little tricky. Find chapter numbers or proper names to give you a hint. Use the source text to understand what the translation means.\n",
    " - Delete everything after that line. It might be line 1922, or line 2157. It won't be exact. In any case, bleualign will do the work of aligning things.\n",
    "\n",
    "Run the below code in your Python IDE. It doesn't work in Jupyter Notebook. Bleualign will calculate the BLEU score of the machine translated segments and will use that BLEU score to predict more accurate alignments. The source and target files will be separate, but they should have the same line count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bleualign.align import Aligner\n",
    "\n",
    "options = {'srcfile': r'6_for_alignment/segmented_en_US.txt',\n",
    "           'targetfile': r'6_for_alignment/segmented_es.txt',\n",
    "           'srctotarget': [r'6_for_alignment/machineTranslated_spa.txt'],\n",
    "           'targettosrc': [],\n",
    "           'output-src': 'output_src.txt',\n",
    "           'output-target': \"output_tgt.txt\", }\n",
    "\n",
    "a = Aligner(options)\n",
    "a.mainloop()\n",
    "output_src, output_target = a.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TMX\n",
    "At this point, it should be possible to create a TMX. There are several ways to do it. One way is to first paste all the text from the aligned files into Excel. Then, you could save as unicode text. Then, upload the tab-delimited file into Okapi Olifant. Save the resulting file as a translation memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
