{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance\n",
    "\n",
    "From [RWS Moravia](https://www.rws.com/insights/rws-moravia-blog/interview-with-an-expert-how-do-you-measure-mt/):\n",
    "\n",
    "Levenshtein distance calculates the difference between the MT output and the post-edited translation. It shows what the post-editor did to the original MT output. Let's say the machine translation output is \"the cat is barking,\" and a post-editor changes this to \"the dog is barking.\" The difference would be six, because you include the three letters deleted and three letters added when editing from \"cat\" to \"dog.\" Then you divide six by the number of letters in the whole segment to come up with a result that is a percentage.\n",
    "\n",
    "# TER\n",
    "\n",
    "From [RWS Moravia](https://www.rws.com/insights/rws-moravia-blog/interview-with-an-expert-how-do-you-measure-mt/):  \n",
    "\n",
    "The second metric we use to measure the human effort of post-editing is the TER score. Whereas the Levenshtein distance counts on a character level—which characters are deleted, added or replaced—the TER score tries to account for the kinds of changes made and makes a calculation based on the number of edits rather than the number of character changes.\n",
    "\n",
    "Again, take the example \"the cat is barking\" and \"the dog is barking.\" The Levenshtein distance counts both the three letters deleted and the three letters added. When you calculate TER, it recognizes a single replacement: one string is replaced with another. That string has a length of three. So, it calculates a single edit with a length of three characters.\n",
    "\n",
    "Therefore, Levenshtein can actually overestimate the effort of making long edits that are in fact only single edits—for example, if you replace one or two characters here and there throughout a long sentence. Levenshtein wouldn't be able to tell the difference in effort between that and overwriting full words. In this case, TER is more reliable because its logic is closer to the actual post-editing effort.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "I don't know of any good Python modules for evaluating TER, but we do have a good method for evaluating Levenshtein edit distance. Note: This particular code seems to give the expected result that TER gives. In other words, the it evaluates the distance between \"the cat is barking\" and \"the dog is barking\" as three, not six.\n",
    "\n",
    "Collect the following:\n",
    "\n",
    "1.  A source text file of at least a couple thousand words.\n",
    "2.  The raw machine translation of the source text\n",
    "3.  The post-edited raw machine translation\n",
    "\n",
    "Requirements for the files to be tested:\n",
    "\n",
    "*   Each file should be a text file, UTF-8 encoded.\n",
    "*   We will perform our Levenshtein calculation on text in the Spanish language.\n",
    "*   Each line should have only one sentence.\n",
    "*   Each file should have exactly the same number of lines.\n",
    "*   Each line should line up with the other lines in the other files.\n",
    "\n",
    "Requirements for our Python environment:\n",
    "\n",
    "Please make sure you have installed pandas and string similarity in Python.\n",
    "\n",
    "To install pandas:\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "To install string similarity:\n",
    "\n",
    "`pip install -U strsimpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary modules\n",
    "\n",
    "We will need pathlib, pandas, and the levenshtein package from similarity. Import them with these statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from similarity.levenshtein import Levenshtein\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "levenshtein = Levenshtein()\n",
    "normalized_levenshtein = NormalizedLevenshtein()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths of files\n",
    "\n",
    "Tell Python where to find the files you'll be evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Path(r\"Levenshtein_files/source.txt\")\n",
    "RAW_MT = Path(r\"Levenshtein_files/raw_mt.txt\")\n",
    "PEMT = Path(r\"Levenshtein_files/post-edited.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a table for results\n",
    "\n",
    "The result list stores rows which contain the results of the BLEU score evaluation. The column headers are indicated by COLUMNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "COLUMNS = (\"Source\",\n",
    "           \"Machine Translation\",\n",
    "           \"Post-edited\",\n",
    "           \"Levenshtein\",\n",
    "           \"Normalized Levenshtein\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the files\n",
    "\n",
    "Now, open each file. You can open multiple files in one line by using the backslash to continue code onto the next line.\n",
    "\n",
    "## Read the lines and evaluate Levenshtein\n",
    "\n",
    "Read all the lines into a tuple called lines. Then, pass that tuple to a `zip` function. When you put an asterisk `*` in front of your function argument, it passes each item in the tuple as its own argument.\n",
    "\n",
    "Levenshtein will give us the number of edits that occured between the `raw_mt` and the `pemt`. However, we also want to evaluate this in terms of the length of the `raw_mt`.\n",
    "\n",
    "Normalized Levenshtein divides the edit distance by the length of the longest segment, giving you a percentage. This is more useful in some ways than a static number of edits.\n",
    "\n",
    "Append each row to the `result` list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files\n",
    "with open(SOURCE, encoding=\"UTF-8\") as a, \\\n",
    "     open(RAW_MT, encoding=\"UTF-8\") as b, \\\n",
    "     open(PEMT, encoding=\"UTF-8\") as c:\n",
    "    \n",
    "    # Read the lines and evaluate Levenshtein\n",
    "    lines = a.readlines(), b.readlines(), c.readlines()\n",
    "    for source, raw_mt, pemt in zip(*lines):\n",
    "        lv = levenshtein.distance(raw_mt, pemt)\n",
    "        norm = normalized_levenshtein.distance(raw_mt, pemt)\n",
    "        \n",
    "        # Append each row to the results list of rows\n",
    "        result.append((source, raw_mt, pemt, lv, norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the results\n",
    "\n",
    "Write an excel file with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Source  \\\n",
      "0  At this stage you are using documented process...   \n",
      "1  Ultimately, however, reports have limited inhe...   \n",
      "2  Some organizations are just getting started an...   \n",
      "3  “To sit back and to see a full year of data, t...   \n",
      "4                                 What’s possible?\\n   \n",
      "\n",
      "                                 Machine Translation  \\\n",
      "0  En esta etapa, usará un proceso de documentaci...   \n",
      "1  Sin embargo, en última instancia, el significa...   \n",
      "2  Algunas organizaciones son incipientes y neces...   \n",
      "3  \"Es impresionante sentarse a examinar los dato...   \n",
      "4                          ¿Qué posibilidades hay?\\n   \n",
      "\n",
      "                                         Post-edited  Levenshtein  \\\n",
      "0  En esta etapa, está utilizando procesos docume...        163.0   \n",
      "1  Sin embargo, en última instancia, los informes...         70.0   \n",
      "2  Algunas organizaciones recién comienzan y nece...         26.0   \n",
      "3  \"Para sentarse y ver un año completo de datos,...        169.0   \n",
      "4                                 ¿Qué es posible?\\n         13.0   \n",
      "\n",
      "   Normalized Levenshtein  \n",
      "0                0.554422  \n",
      "1                0.409357  \n",
      "2                0.200000  \n",
      "3                0.584775  \n",
      "4                0.541667  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(result, columns=COLUMNS)\n",
    "df.to_excel(\"Levenshtein.xlsx\", index=False)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file and view the results. Set the Levenshtein rate to a percentage. What do you think? Are these the scores you expected? Remember, it's a good idea to track the time spent on the post-editing task. The edit distance only tells you half the story. Although an edit may be minimal, a disproportionate amount of time may have been required to make it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
