{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "**From KantanMT**: BLEU Score is quick to use, inexpensive to operate, language independent, and correlates highly with human evaluation. It is the most widely used automated method of determining the quality of machine translation. \n",
    "\n",
    "The BLEU metric scores a translation on a scale of 0 to 1, but is frequently displayed as a percentage value. The closer to 1, the more the translation correlates to a human translation. The BLEU score is the proportion of words that appear in MT which also exist in the human translation (the \"golden reference\"). Put simply, the BLEU metric measures how many words overlap in a given translation when compared to a reference translation, giving higher scores to sequential words.\n",
    "\n",
    "What is a good BLEU Score? BLEU scores range from 0-100%. A score less than 15% means that your machine translation engine is not performing optimally and a high level of post-editing will be required to finalise your translations and reach publishable quality.\n",
    "\n",
    "A score greater than 50% is a very good score and significantly less post-editing will be required to achieve publishable translation quality.\n",
    "\n",
    "Improving BLEU Score. There is a high correlation between the number of words used in training a machine translation engine and its BLEU score. Put simply, the more training data that is uploaded, the better the BLEU score and consequently the generated translations.\n",
    "\n",
    "BLEU has some important limitations that are described by Rachael Tatman [here](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213).\n",
    "\n",
    "*   It doesn't consider meaning\n",
    "*   It doesn't directly consider sentence structure\n",
    "*   It doesn't handle morphologically rich languages well\n",
    "*   It doesn't map well to human judgements\n",
    "\n",
    "Only use BLEU if:\n",
    "\n",
    "*   You're doing machine translation AND\n",
    "*   You're evaluating across an entire corpus AND\n",
    "*   You know the limitations of the metric and you're prepared to accept them.\n",
    "\n",
    "## METEOR\n",
    "\n",
    "From [RWS Moravia](https://www.rws.com/insights/rws-moravia-blog/interview-with-an-expert-how-do-you-measure-mt/):\n",
    "\n",
    "Then there is a metric called METEOR. METEOR's algorithm is more nuanced because not only does it compare MT and the human reference in both directions, it also takes into account things like linguistics. While BLEU checks existing words exactly as they appear, METEOR considers some linguistic variants. In English, \"ride\" or \"riding\" would count as two different words for the BLEU score. But for METEOR, it would count as a single word because they have the same root.\n",
    "\n",
    "That's why we generally use METEOR in more cases than we use BLEU. These nuances can affect the accuracy of the quality measurement.\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "Collect the following:\n",
    "\n",
    "1.  A source text file of at least a couple thousand words.\n",
    "2.  The 'candidate' raw machine translation of the source text\n",
    "3.  A 'reference' human translation of the source text\n",
    "\n",
    "Requirements for the files to be tested:\n",
    "\n",
    "*   Each file should be a text file, UTF-8 encoded.\n",
    "*   We will perform our BLEU score evaluation using the Spanish language. BLEU score evaluation of Chinese is a little bit different and requires some other tools.\n",
    "*   Each line should have only one sentence.\n",
    "*   Each file should have exactly the same number of lines.\n",
    "*   Each line should line up with the other lines in the other files.\n",
    "\n",
    "Requirements for our Python environment:\n",
    "\n",
    "Please make sure you have installed pandas and nltk in Python.\n",
    "\n",
    "To install pandas:\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "To install nltk:\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "To install the nltk punctuation data:\n",
    "\n",
    "`python -m nltk.downloader punkt`\n",
    "\n",
    "## Importing necessary modules\n",
    "\n",
    "We will need pathlib, pandas, and several nltk modules. Import them with these statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths of files\n",
    "\n",
    "Tell Python where to find the files you'll be evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Path(r\"source.txt\")\n",
    "CANDIDATE = Path(r\"candidate.txt\")\n",
    "REFERENCE = Path(r\"reference.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a table for results\n",
    "\n",
    "The result list stores rows which contain the results of the BLEU score evaluation. The column headers are indicated by COLUMNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "COLUMNS = (\"Source\",\n",
    "           \"Reference\",\n",
    "           \"Candidate\",\n",
    "           \"BLEU\",\n",
    "           \"METEOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the files\n",
    "\n",
    "Now, open each file. You can open multiple files in one line by using the backslash to continue code onto the next line.\n",
    "\n",
    "**Note**: For the purposes of this demonstration, I'm using Baidu as my MT candidate and Google as my reference translation. A real evaluation would use a human translation as the reference translation.\n",
    "\n",
    "## Read the lines and tokenize the content\n",
    "\n",
    "Read all the lines into a tuple called lines. Then, pass that tuple to a `zip` function. When you put an asterisk `*` in front of your function argument, it passes each item in the tuple as its own argument.\n",
    "\n",
    "Before we evaluate the BLEU score, we need to tokenize the text. Essentially, tokenization intelligently splits the text into words.\n",
    "\n",
    "## Get the BLEU score and METEOR score\n",
    "\n",
    "We will be using `sentence_bleu` for our example. The first parameter is for reference texts. The first parameter expects a list of one or more lists of tokens. We've already tokenized our reference sentence as `ref_token`.\n",
    "\n",
    "The next parameter is for the machine translation candidate. This parameter expects a list of tokens. We have tokenized our candidate as `can_token`.\n",
    "\n",
    "Next, we will use a smoothing function to calculate a score even when there are no direct n-gram overlaps. For more details, look up `nltk` smoothing functions.\n",
    "\n",
    "The `meteor_score` function can accept as little as two parameters: a list of reference sentences (not tokens!) and the candidate sentence (not tokens!)\n",
    "\n",
    "Append each row to the `result` list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files\n",
    "with open(SOURCE, encoding=\"UTF-8\") as a, \\\n",
    "     open(CANDIDATE, encoding=\"UTF-8\") as b, \\\n",
    "     open(REFERENCE, encoding=\"UTF-8\") as c:\n",
    "     \n",
    "    # Read the lines and tokenize the content\n",
    "    lines = a.readlines(), b.readlines(), c.readlines()\n",
    "    for source, candidate, reference in zip(*lines):\n",
    "        src_token = word_tokenize(source)\n",
    "        ref_token = word_tokenize(reference)\n",
    "        can_token = word_tokenize(candidate)\n",
    "        \n",
    "        # Get the BLEU score and METEOR score\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu = sentence_bleu([ref_token], can_token, smoothing_function=smoothing)\n",
    "        meteor = meteor_score([reference], candidate)\n",
    "        \n",
    "        # Append to result list\n",
    "        result.append([source, reference, candidate, bleu, meteor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the results\n",
    "\n",
    "Outside the loop, convert our list into a dataframe with the specified columns and write it to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Source  \\\n",
      "0  At this stage you are using documented process...   \n",
      "1  Ultimately, however, reports have limited inhe...   \n",
      "2  Some organizations are just getting started an...   \n",
      "3  “To sit back and to see a full year of data, t...   \n",
      "4                                 What’s possible?\\n   \n",
      "\n",
      "                                           Reference  \\\n",
      "0  En esta etapa, está utilizando procesos docume...   \n",
      "1  Sin embargo, en última instancia, los informes...   \n",
      "2  Algunas organizaciones recién comienzan y nece...   \n",
      "3  \"Para sentarse y ver un año completo de datos,...   \n",
      "4                                 ¿Qué es posible?\\n   \n",
      "\n",
      "                                           Candidate      BLEU    METEOR  \n",
      "0  En esta etapa, usará un proceso de documentaci...  0.155455  0.456349  \n",
      "1  Sin embargo, en última instancia, el significa...  0.320016  0.574743  \n",
      "2  Algunas organizaciones son incipientes y neces...  0.409318  0.722388  \n",
      "3  \"Es impresionante sentarse a examinar los dato...  0.130856  0.347261  \n",
      "4                          ¿Qué posibilidades hay?\\n  0.095544  0.166667  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(result, columns=COLUMNS)\n",
    "df.to_excel(\"BLEU.xlsx\", index=False)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file and view the results. Set the bleu score to a percentage. What do you think? Are these the scores you expected? What makes the scores higher or lower?\n",
    "\n",
    "Without a pair of human eyes, numbers are just numbers. I recommend pairing a BLEU/METEOR evaluation done by a computer with a [TAUS Adequacy/Fluency](https://www.taus.net/academy/best-practices/evaluate-best-practices/adequacy-fluency-guidelines) evaluation done by a human.\n",
    "\n",
    "![TAUS Adequacy/Fluency](screenshots\\taus_adequacy_fluency.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
